{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "        L = 1/(2N) * e^T * e\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    return 1/(2*N) * e.T.dot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            w = np.array([w0, w1])\n",
    "            losses[i, j] = compute_loss(y, tx, w)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=67.43848959151069, w0*=75.0, w1*=12.5, execution time=0.001 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=25)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    return -1/N * tx.T.dot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.28212470159644, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.64235561651283, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.97477639885523, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889339, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670433\n",
      "GD iter. 13/49: loss=65.93073010267464, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260339, w0=74.06780582623098, w1=11.034894861713955\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260339, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.06780585492008, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.03489486598882\n",
      "GD iter. 26/49: loss=65.93073010260339, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.0678058549263, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba6c7f816ce4ba6968616a1a0433ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    return -1/N * tx.T.dot(e)\n",
    "\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/99: loss=3673.6851104397742, w0=8.546642058131159, w1=5.533232529126314\n",
      "SGD iter. 1/99: loss=1601.1489144719014, w0=14.175897954930122, w1=2.4962181600957902\n",
      "SGD iter. 2/99: loss=1193.723809154543, w0=19.01391212348605, w1=-0.2761030809988618\n",
      "SGD iter. 3/99: loss=1138.3365714979916, w0=23.677780405183416, w1=-0.9650682043057994\n",
      "SGD iter. 4/99: loss=1778.6994053459844, w0=29.42705408505348, w1=2.956322234301637\n",
      "SGD iter. 5/99: loss=1539.7050738934142, w0=34.94811247767524, w1=6.926685018043653\n",
      "SGD iter. 6/99: loss=481.7539306877305, w0=38.018876311377056, w1=6.094130896271562\n",
      "SGD iter. 7/99: loss=354.04687972181534, w0=40.62257805206152, w1=4.3501809673047145\n",
      "SGD iter. 8/99: loss=937.2446451553795, w0=44.90709402647466, w1=8.862118892371367\n",
      "SGD iter. 9/99: loss=572.5415168861974, w0=48.249549833786396, w1=11.457769601599637\n",
      "SGD iter. 10/99: loss=207.65582587976985, w0=50.26045333829381, w1=9.992985133411638\n",
      "SGD iter. 11/99: loss=176.40204439125435, w0=52.07499442830681, w1=9.70416334822685\n",
      "SGD iter. 12/99: loss=102.33158459987493, w0=53.45057284660236, w1=9.018821594117032\n",
      "SGD iter. 13/99: loss=408.95134081082426, w0=56.27480225125311, w1=11.2701494960367\n",
      "SGD iter. 14/99: loss=65.84195468869311, w0=57.34001159651196, w1=10.528026741325526\n",
      "SGD iter. 15/99: loss=231.7561186371935, w0=59.440671379377285, w1=11.960167855290269\n",
      "SGD iter. 16/99: loss=131.73873241456283, w0=60.99894519820338, w1=12.680915710858674\n",
      "SGD iter. 17/99: loss=170.2485592785863, w0=62.74822218432278, w1=13.823924544396386\n",
      "SGD iter. 18/99: loss=726.9257163719294, w0=64.63782442687226, w1=7.65374075606765\n",
      "SGD iter. 19/99: loss=12.633053258844232, w0=64.7175480294402, w1=7.871746187010397\n",
      "SGD iter. 20/99: loss=166.29808285161195, w0=66.37750298836777, w1=9.20057458417348\n",
      "SGD iter. 21/99: loss=117.57604872893063, w0=67.83943688535953, w1=10.389114283578115\n",
      "SGD iter. 22/99: loss=13.69800914506505, w0=67.80991371514254, w1=10.433135142037761\n",
      "SGD iter. 23/99: loss=12.437794252140627, w0=67.79973671073715, w1=10.359547234195347\n",
      "SGD iter. 24/99: loss=10.386675291738147, w0=67.83153072067526, w1=10.441344949161081\n",
      "SGD iter. 25/99: loss=12.446186090696012, w0=67.81964900402565, w1=10.368580549581422\n",
      "SGD iter. 26/99: loss=88.7284777252105, w0=68.96877047595707, w1=11.254322199360772\n",
      "SGD iter. 27/99: loss=10.293812803767143, w0=68.88645321044989, w1=11.284191500503766\n",
      "SGD iter. 28/99: loss=10.797219677304415, w0=68.84535980815352, w1=11.415357978387664\n",
      "SGD iter. 29/99: loss=13.459064449204469, w0=68.78631235991304, w1=11.353970557339897\n",
      "SGD iter. 30/99: loss=9.631504609691303, w0=68.72798514631279, w1=11.539262289598904\n",
      "SGD iter. 31/99: loss=9.303049778214957, w0=68.75709103949706, w1=11.593255723666008\n",
      "SGD iter. 32/99: loss=41.25225710611585, w0=69.5625069202628, w1=12.275328579839021\n",
      "SGD iter. 33/99: loss=7.419688633737032, w0=69.33072790319962, w1=12.37342031459564\n",
      "SGD iter. 34/99: loss=10.550727879938515, w0=69.3135146305286, w1=12.32982817941856\n",
      "SGD iter. 35/99: loss=28.96155685060795, w0=70.02755372521943, w1=12.92960678322372\n",
      "SGD iter. 36/99: loss=32.514596088715436, w0=70.70770170790158, w1=13.399000805629314\n",
      "SGD iter. 37/99: loss=11.659198773869154, w0=70.80981184250001, w1=13.168431786800042\n",
      "SGD iter. 38/99: loss=15.906771541314853, w0=71.18874471477409, w1=13.392855898753261\n",
      "SGD iter. 39/99: loss=23.1204886748718, w0=71.7976215959343, w1=13.709236360668026\n",
      "SGD iter. 40/99: loss=13.081309706412918, w0=71.384985062831, w1=13.825955119996854\n",
      "SGD iter. 41/99: loss=13.65990403316111, w0=71.11144959086842, w1=13.971034497369482\n",
      "SGD iter. 42/99: loss=21.845379584262105, w0=71.69270392178959, w1=14.254837367135092\n",
      "SGD iter. 43/99: loss=33.30280368695212, w0=72.28013728669424, w1=14.637275755188597\n",
      "SGD iter. 44/99: loss=8.782753941820081, w0=72.42021788900843, w1=14.695096673649022\n",
      "SGD iter. 45/99: loss=14.29296695256977, w0=72.84796812167342, w1=14.900017646233673\n",
      "SGD iter. 46/99: loss=13.345143518767731, w0=73.20399331287565, w1=15.073948317836786\n",
      "SGD iter. 47/99: loss=22.52051964659043, w0=73.57164446119008, w1=15.287434725722816\n",
      "SGD iter. 48/99: loss=4.916267232454054, w0=73.68284383038558, w1=15.22963278720793\n",
      "SGD iter. 49/99: loss=10.32969018085085, w0=73.36989768789131, w1=15.377054996033563\n",
      "SGD iter. 50/99: loss=15.668023986603243, w0=73.30870993671466, w1=15.147217797681417\n",
      "SGD iter. 51/99: loss=5.1994247094034876, w0=73.4564161970775, w1=15.123405245265026\n",
      "SGD iter. 52/99: loss=12.634543946377006, w0=73.75140047291632, w1=15.344598518458119\n",
      "SGD iter. 53/99: loss=11.453780520764736, w0=73.95048511980904, w1=15.487461653316164\n",
      "SGD iter. 54/99: loss=11.337175938064508, w0=73.93283146210568, w1=15.536792163772793\n",
      "SGD iter. 55/99: loss=11.362007138338155, w0=74.13947548160911, w1=15.436129541926343\n",
      "SGD iter. 56/99: loss=10.84776225584719, w0=74.33181023898634, w1=15.334111188721929\n",
      "SGD iter. 57/99: loss=21.710019453407686, w0=74.00849010700225, w1=15.395284911588742\n",
      "SGD iter. 58/99: loss=14.893123613072504, w0=73.5930698450626, w1=15.51295945369561\n",
      "SGD iter. 59/99: loss=14.30173832385703, w0=73.43381058319184, w1=15.557183591435065\n",
      "SGD iter. 60/99: loss=19.337096150975018, w0=73.37466056294444, w1=15.370681710354644\n",
      "SGD iter. 61/99: loss=15.480946099874428, w0=73.60002404603307, w1=15.499538702557441\n",
      "SGD iter. 62/99: loss=15.896026987165332, w0=73.93337528745826, w1=15.480956395053894\n",
      "SGD iter. 63/99: loss=8.444423741438873, w0=73.91874997820598, w1=15.409841688818178\n",
      "SGD iter. 64/99: loss=11.05151615009163, w0=74.09631648893678, w1=15.533987179436307\n",
      "SGD iter. 65/99: loss=20.911538623230285, w0=73.81230393440819, w1=15.556230749969\n",
      "SGD iter. 66/99: loss=8.381195074160173, w0=74.05422508262687, w1=15.64175722131668\n",
      "SGD iter. 67/99: loss=9.87046789773798, w0=74.1603231733275, w1=15.67486891374501\n",
      "SGD iter. 68/99: loss=7.524305703983318, w0=74.36101950918363, w1=15.735488282348218\n",
      "SGD iter. 69/99: loss=9.00557228710603, w0=74.32079137212375, w1=15.632836169359896\n",
      "SGD iter. 70/99: loss=4.815079003978604, w0=74.33191691624974, w1=15.48391724273283\n",
      "SGD iter. 71/99: loss=7.735782133307481, w0=74.49862104651176, w1=15.473198130762844\n",
      "SGD iter. 72/99: loss=8.806584213124655, w0=74.45844166979121, w1=15.381935939314763\n",
      "SGD iter. 73/99: loss=12.557264570881582, w0=74.29744800996968, w1=14.98670814522356\n",
      "SGD iter. 74/99: loss=10.548110651528157, w0=73.98607863472606, w1=15.079766713358024\n",
      "SGD iter. 75/99: loss=20.77118336191588, w0=73.67727917101705, w1=15.139596222666558\n",
      "SGD iter. 76/99: loss=9.148129448205015, w0=73.9552587507304, w1=15.253734917265811\n",
      "SGD iter. 77/99: loss=15.405829100510397, w0=73.72631514924662, w1=15.202327206253063\n",
      "SGD iter. 78/99: loss=10.281586460372175, w0=73.97499598623992, w1=15.298218201066124\n",
      "SGD iter. 79/99: loss=12.942461542813149, w0=73.56560106484855, w1=15.48270198131394\n",
      "SGD iter. 80/99: loss=668.877353147296, w0=74.5428908483128, w1=9.955967196843112\n",
      "SGD iter. 81/99: loss=20.57200424829132, w0=75.04848976337675, w1=10.369764837315333\n",
      "SGD iter. 82/99: loss=37.95276692565076, w0=74.27129667069582, w1=11.052791352130031\n",
      "SGD iter. 83/99: loss=17.613029880920386, w0=74.64566768920638, w1=11.3798498235918\n",
      "SGD iter. 84/99: loss=27.110887278336648, w0=74.09352440212248, w1=11.89384964240411\n",
      "SGD iter. 85/99: loss=29.75328482289283, w0=73.45025337604582, w1=12.299868000720432\n",
      "SGD iter. 86/99: loss=24.449326070192587, w0=72.9605019309451, w1=12.596663605016857\n",
      "SGD iter. 87/99: loss=13.50078120288319, w0=73.26078716417571, w1=12.814104388474515\n",
      "SGD iter. 88/99: loss=14.634051988430953, w0=72.82717682290964, w1=13.1535162416313\n",
      "SGD iter. 89/99: loss=16.820755239642416, w0=73.29484981947085, w1=13.399436019791013\n",
      "SGD iter. 90/99: loss=9.319968442540896, w0=72.98051216098067, w1=13.656412591347138\n",
      "SGD iter. 91/99: loss=27.41312546520144, w0=73.45100532616422, w1=14.001086159468754\n",
      "SGD iter. 92/99: loss=24.26391034038997, w0=73.85487709647859, w1=14.29023055460122\n",
      "SGD iter. 93/99: loss=10.376576507395582, w0=73.92261571821192, w1=14.247385492302651\n",
      "SGD iter. 94/99: loss=16.57824429562174, w0=73.61584327413988, w1=14.441568889884692\n",
      "SGD iter. 95/99: loss=11.550563240968785, w0=73.92466709829476, w1=14.598329449035672\n",
      "SGD iter. 96/99: loss=8.131263754889357, w0=73.9419051125028, w1=14.598457483584623\n",
      "SGD iter. 97/99: loss=12.481444652003104, w0=73.8707679040758, w1=14.488897197476351\n",
      "SGD iter. 98/99: loss=14.564817115695426, w0=73.71328068584015, w1=14.346988345834236\n",
      "SGD iter. 99/99: loss=21.780975837939643, w0=73.12804922823024, w1=14.49940502570869\n",
      "SGD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.1\n",
    "batch_size = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb5e830034946a6909f854130e43d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=101, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.28212470159644, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.64235561651283, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.97477639885523, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889339, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670433\n",
      "GD iter. 13/49: loss=65.93073010267464, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260339, w0=74.06780582623098, w1=11.034894861713955\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260339, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.06780585492008, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.03489486598882\n",
      "GD iter. 26/49: loss=65.93073010260339, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.0678058549263, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd62b31806b4dc2bed37c1b57e40f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    subgradient = -1/N * tx.T.dot(np.sign(e))\n",
    "    return subgradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_MAE(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "        L = 1/(2N) * e^T * e\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    e = y - tx.dot(w)\n",
    "    return 1/N * np.sum(np.abs(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * subgradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492639, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926353, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.96780585492635, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.817212322770175, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657148, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512217, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605637, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.423218888756113, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.49504179464643, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313204, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.728084326668132, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158963, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221255, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751498, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.7052797283770005, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269659, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790258, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993147, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022583, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517474, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643112, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951546, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.314707697380741, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.313052246871384, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024388, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.31163893648421, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062871, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.31154967725576, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.3115050476415355, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605974, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.3113488439917464, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956186, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149073, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920623, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.31092486265661, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.31089223718627, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.3106844802203375, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.310592228440201, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099028, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.310599343585063, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.31061357387479, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.310574966149887, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.310583639649728, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.31057896067014, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.310578336960181, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534204, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749845, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.31057734507262, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481998, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.310578699848581, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409099, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112924, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976985, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997397, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017809, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.31057770796102, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926684, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.310626099944344, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107792, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.310583460535291, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383832, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555703, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.3105747000391235, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419766, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676496, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260848, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.310579425625379, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229538, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996193, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.310575393556711, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961859, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.3105750629275255, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993001, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737818, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862547, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882957, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.310578103108631, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903371, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.31058236353638, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.3106257484553465, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040678, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.310579490143804, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979161, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064516, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.3105831918636355, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851097, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504004, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652995, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.310575669673408, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.3105795224030174, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368683, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.31062627038102, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527843, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705161, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326828, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075976, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481123, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.31057819988627, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231714, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697522, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980251, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844346, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.310577207998708, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282852, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.31057833376961, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.009 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed871121e6be426e87c20d727a061124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            subgradient = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_loss_MAE(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "            w = w - gamma * subgradient\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=62.489254960959904, w0=0.7, w1=0.05630961504530349\n",
      "SubSGD iter. 1/499: loss=64.18614133257935, w0=1.4, w1=0.03101184965646216\n",
      "SubSGD iter. 2/499: loss=68.59151101784136, w0=2.0999999999999996, w1=-0.39931973880427385\n",
      "SubSGD iter. 3/499: loss=48.22216000099108, w0=2.8, w1=-1.2407382485302185\n",
      "SubSGD iter. 4/499: loss=78.50582590400836, w0=3.5, w1=-0.9911397858430403\n",
      "SubSGD iter. 5/499: loss=107.91276492353147, w0=4.2, w1=0.13399017428726678\n",
      "SubSGD iter. 6/499: loss=87.99287969400567, w0=4.9, w1=1.1903914056661566\n",
      "SubSGD iter. 7/499: loss=82.87899452130608, w0=5.6000000000000005, w1=1.9154115957469608\n",
      "SubSGD iter. 8/499: loss=78.42028887732764, w0=6.300000000000001, w1=2.5345449916916274\n",
      "SubSGD iter. 9/499: loss=87.9661229089701, w0=7.000000000000001, w1=3.5469833295580484\n",
      "SubSGD iter. 10/499: loss=68.30795341742721, w0=7.700000000000001, w1=3.5145543217906887\n",
      "SubSGD iter. 11/499: loss=77.63422480792546, w0=8.4, w1=4.275911272409758\n",
      "SubSGD iter. 12/499: loss=48.34328982972341, w0=9.1, w1=3.6531248655894655\n",
      "SubSGD iter. 13/499: loss=77.58544042250867, w0=9.799999999999999, w1=4.821455666106934\n",
      "SubSGD iter. 14/499: loss=51.688830727475434, w0=10.499999999999998, w1=4.027086321616154\n",
      "SubSGD iter. 15/499: loss=64.47488242329415, w0=11.199999999999998, w1=4.38211379914598\n",
      "SubSGD iter. 16/499: loss=61.17488763536285, w0=11.899999999999997, w1=4.19864802009556\n",
      "SubSGD iter. 17/499: loss=67.77762513010525, w0=12.599999999999996, w1=4.46878647038523\n",
      "SubSGD iter. 18/499: loss=78.92395138821792, w0=13.299999999999995, w1=5.831537865776976\n",
      "SubSGD iter. 19/499: loss=63.89522613998474, w0=13.999999999999995, w1=5.590557951110328\n",
      "SubSGD iter. 20/499: loss=57.11248446286643, w0=14.699999999999994, w1=4.903863315604191\n",
      "SubSGD iter. 21/499: loss=68.33207335214837, w0=15.399999999999993, w1=5.649848969191588\n",
      "SubSGD iter. 22/499: loss=46.244101387991016, w0=16.099999999999994, w1=5.079888844655602\n",
      "SubSGD iter. 23/499: loss=43.66057165206227, w0=16.799999999999994, w1=4.544369161829397\n",
      "SubSGD iter. 24/499: loss=55.33400413894684, w0=17.499999999999993, w1=5.149772313273381\n",
      "SubSGD iter. 25/499: loss=67.64911047970028, w0=18.199999999999992, w1=5.795919546660602\n",
      "SubSGD iter. 26/499: loss=45.46473824383369, w0=18.89999999999999, w1=5.551892721951451\n",
      "SubSGD iter. 27/499: loss=60.20331914266395, w0=19.59999999999999, w1=5.5891800335693205\n",
      "SubSGD iter. 28/499: loss=56.29497461270476, w0=20.29999999999999, w1=5.239242102089003\n",
      "SubSGD iter. 29/499: loss=41.92004282910807, w0=20.99999999999999, w1=4.712285066320155\n",
      "SubSGD iter. 30/499: loss=57.54919254145588, w0=21.69999999999999, w1=5.670420157221963\n",
      "SubSGD iter. 31/499: loss=63.59305377283622, w0=22.399999999999988, w1=6.7542955539132645\n",
      "SubSGD iter. 32/499: loss=41.014332408489615, w0=23.099999999999987, w1=5.954464819850555\n",
      "SubSGD iter. 33/499: loss=61.30632499848275, w0=23.799999999999986, w1=6.600612053237776\n",
      "SubSGD iter. 34/499: loss=60.05275075813169, w0=24.499999999999986, w1=7.684487449929078\n",
      "SubSGD iter. 35/499: loss=36.23415920728479, w0=25.199999999999985, w1=7.137325147695208\n",
      "SubSGD iter. 36/499: loss=52.47109172689758, w0=25.899999999999984, w1=7.741857253549367\n",
      "SubSGD iter. 37/499: loss=35.16403053385116, w0=26.599999999999984, w1=7.183733549800327\n",
      "SubSGD iter. 38/499: loss=48.489504664704825, w0=27.299999999999983, w1=7.472230814357743\n",
      "SubSGD iter. 39/499: loss=53.08739359445683, w0=27.999999999999982, w1=7.625124571472324\n",
      "SubSGD iter. 40/499: loss=51.9471305526446, w0=28.69999999999998, w1=8.239038984849907\n",
      "SubSGD iter. 41/499: loss=34.1784758985426, w0=29.39999999999998, w1=7.827963800307681\n",
      "SubSGD iter. 42/499: loss=44.74207614024374, w0=30.09999999999998, w1=7.57253191920611\n",
      "SubSGD iter. 43/499: loss=59.931729519115486, w0=30.79999999999998, w1=7.917000166076577\n",
      "SubSGD iter. 44/499: loss=29.03226421025687, w0=31.49999999999998, w1=7.438951963650454\n",
      "SubSGD iter. 45/499: loss=43.26052567517323, w0=32.19999999999998, w1=8.096134666845439\n",
      "SubSGD iter. 46/499: loss=32.74753835519047, w0=32.899999999999984, w1=7.296303932782729\n",
      "SubSGD iter. 47/499: loss=27.59849229338059, w0=33.59999999999999, w1=6.7878028150428955\n",
      "SubSGD iter. 48/499: loss=48.38151411561069, w0=34.29999999999999, w1=7.5128230051237\n",
      "SubSGD iter. 49/499: loss=44.76951125677501, w0=34.99999999999999, w1=8.131956401068367\n",
      "SubSGD iter. 50/499: loss=49.84016086562769, w0=35.699999999999996, w1=8.988178453572905\n",
      "SubSGD iter. 51/499: loss=43.13062912100869, w0=36.4, w1=10.044579684951795\n",
      "SubSGD iter. 52/499: loss=30.43942452168047, w0=37.1, w1=9.64914289300879\n",
      "SubSGD iter. 53/499: loss=31.530616471366997, w0=37.800000000000004, w1=9.448393355959539\n",
      "SubSGD iter. 54/499: loss=35.31581781264203, w0=38.50000000000001, w1=9.500553701489249\n",
      "SubSGD iter. 55/499: loss=27.226265134145702, w0=39.20000000000001, w1=8.716320973449925\n",
      "SubSGD iter. 56/499: loss=45.793677739635584, w0=39.90000000000001, w1=8.960633697159059\n",
      "SubSGD iter. 57/499: loss=33.43192151084517, w0=40.600000000000016, w1=9.617816400354045\n",
      "SubSGD iter. 58/499: loss=23.875835934760282, w0=41.30000000000002, w1=9.112798668529722\n",
      "SubSGD iter. 59/499: loss=31.889064100535222, w0=42.00000000000002, w1=9.769981371724707\n",
      "SubSGD iter. 60/499: loss=30.2960348671684, w0=42.700000000000024, w1=10.105209427027962\n",
      "SubSGD iter. 61/499: loss=26.485318347887397, w0=43.40000000000003, w1=9.334386360959774\n",
      "SubSGD iter. 62/499: loss=38.46286897313665, w0=44.10000000000003, w1=9.950773309797023\n",
      "SubSGD iter. 63/499: loss=22.46666478828668, w0=44.80000000000003, w1=10.211485531178887\n",
      "SubSGD iter. 64/499: loss=22.881154435467174, w0=45.500000000000036, w1=9.000062643252708\n",
      "SubSGD iter. 65/499: loss=24.79216255757558, w0=46.20000000000004, w1=8.146216372827439\n",
      "SubSGD iter. 66/499: loss=27.107483205368858, w0=46.90000000000004, w1=7.406883573383495\n",
      "SubSGD iter. 67/499: loss=34.200722051071, w0=47.600000000000044, w1=8.168240524002565\n",
      "SubSGD iter. 68/499: loss=33.3758790162443, w0=48.30000000000005, w1=9.009430692227207\n",
      "SubSGD iter. 69/499: loss=30.235508589286177, w0=49.00000000000005, w1=9.205950775005709\n",
      "SubSGD iter. 70/499: loss=14.680477429337103, w0=49.70000000000005, w1=8.593152604004297\n",
      "SubSGD iter. 71/499: loss=30.41163547681758, w0=50.400000000000055, w1=9.3181727940851\n",
      "SubSGD iter. 72/499: loss=107.93831712952655, w0=51.10000000000006, w1=5.945029116495423\n",
      "SubSGD iter. 73/499: loss=36.88427188435274, w0=51.80000000000006, w1=6.292996162885838\n",
      "SubSGD iter. 74/499: loss=30.479379071032383, w0=52.500000000000064, w1=7.461326963403306\n",
      "SubSGD iter. 75/499: loss=24.330882075638023, w0=53.20000000000007, w1=7.111389031922989\n",
      "SubSGD iter. 76/499: loss=37.797145880151334, w0=53.90000000000007, w1=8.4767584845866\n",
      "SubSGD iter. 77/499: loss=35.17254369351345, w0=54.60000000000007, w1=9.489436825939368\n",
      "SubSGD iter. 78/499: loss=18.532788992248996, w0=55.300000000000075, w1=9.754221935130136\n",
      "SubSGD iter. 79/499: loss=6.737521092780909, w0=56.00000000000008, w1=8.628289521628965\n",
      "SubSGD iter. 80/499: loss=8.941268889692594, w0=56.70000000000008, w1=7.416866633702784\n",
      "SubSGD iter. 81/499: loss=3.8860726764884674, w0=57.400000000000084, w1=6.9083655159629505\n",
      "SubSGD iter. 82/499: loss=20.401071200916306, w0=58.10000000000009, w1=7.500683477231129\n",
      "SubSGD iter. 83/499: loss=15.312219237513943, w0=58.80000000000009, w1=7.373669032003104\n",
      "SubSGD iter. 84/499: loss=23.85575441595408, w0=59.50000000000009, w1=8.457544428694405\n",
      "SubSGD iter. 85/499: loss=2.720442675165849, w0=60.200000000000095, w1=7.832728328883459\n",
      "SubSGD iter. 86/499: loss=22.98515813397725, w0=60.9000000000001, w1=8.449115277720708\n",
      "SubSGD iter. 87/499: loss=10.645674220402064, w0=61.6000000000001, w1=8.338941160839967\n",
      "SubSGD iter. 88/499: loss=13.498888756842831, w0=62.300000000000104, w1=7.908609572379231\n",
      "SubSGD iter. 89/499: loss=17.529933923255953, w0=63.00000000000011, w1=8.654595225966629\n",
      "SubSGD iter. 90/499: loss=13.624110139175343, w0=63.70000000000011, w1=9.285760565961482\n",
      "SubSGD iter. 91/499: loss=4.614415482241178, w0=64.4000000000001, w1=8.915492475432686\n",
      "SubSGD iter. 92/499: loss=0.9966963459747262, w0=63.7000000000001, w1=9.355687730139657\n",
      "SubSGD iter. 93/499: loss=0.026881047885531473, w0=63.0000000000001, w1=9.740971870467785\n",
      "SubSGD iter. 94/499: loss=16.947385092369117, w0=63.7000000000001, w1=10.168488955685204\n",
      "SubSGD iter. 95/499: loss=15.261672629417028, w0=64.4000000000001, w1=9.786362971134869\n",
      "SubSGD iter. 96/499: loss=7.413251706070696, w0=65.10000000000011, w1=9.82868952773836\n",
      "SubSGD iter. 97/499: loss=11.278151271971694, w0=65.80000000000011, w1=10.997020328255829\n",
      "SubSGD iter. 98/499: loss=18.900391955442274, w0=66.50000000000011, w1=11.660164645298876\n",
      "SubSGD iter. 99/499: loss=7.556404487971193, w0=67.20000000000012, w1=11.106057571302294\n",
      "SubSGD iter. 100/499: loss=3.9339877301901183, w0=66.50000000000011, w1=11.614558689042127\n",
      "SubSGD iter. 101/499: loss=11.304137204168015, w0=67.20000000000012, w1=11.811078771820629\n",
      "SubSGD iter. 102/499: loss=5.763331248228241, w0=67.90000000000012, w1=12.512012769255382\n",
      "SubSGD iter. 103/499: loss=5.258914331301433, w0=68.60000000000012, w1=12.158897017688068\n",
      "SubSGD iter. 104/499: loss=6.252064166963194, w0=67.90000000000012, w1=12.590689595971488\n",
      "SubSGD iter. 105/499: loss=9.503759720564751, w0=68.60000000000012, w1=12.063500957224262\n",
      "SubSGD iter. 106/499: loss=0.4201417574795414, w0=67.90000000000012, w1=11.639959704170247\n",
      "SubSGD iter. 107/499: loss=7.633083689493745, w0=68.60000000000012, w1=11.384527823068677\n",
      "SubSGD iter. 108/499: loss=4.582812933799104, w0=69.30000000000013, w1=11.3659483641698\n",
      "SubSGD iter. 109/499: loss=7.455057352822934, w0=70.00000000000013, w1=11.381256030881936\n",
      "SubSGD iter. 110/499: loss=4.165576147768164, w0=70.70000000000013, w1=12.012421370876789\n",
      "SubSGD iter. 111/499: loss=6.230965091491093, w0=70.00000000000013, w1=12.570545074625828\n",
      "SubSGD iter. 112/499: loss=7.72432507457011, w0=69.30000000000013, w1=12.842025256608407\n",
      "SubSGD iter. 113/499: loss=2.588766515193761, w0=68.60000000000012, w1=13.54553017499927\n",
      "SubSGD iter. 114/499: loss=3.2844956056575185, w0=67.90000000000012, w1=14.09269247723314\n",
      "SubSGD iter. 115/499: loss=0.05845796831031436, w0=68.60000000000012, w1=13.479894306231728\n",
      "SubSGD iter. 116/499: loss=10.310033076255145, w0=69.30000000000013, w1=12.955261381519989\n",
      "SubSGD iter. 117/499: loss=9.408959550904811, w0=70.00000000000013, w1=12.99254869313786\n",
      "SubSGD iter. 118/499: loss=5.554991207125553, w0=70.70000000000013, w1=13.717568883218663\n",
      "SubSGD iter. 119/499: loss=0.04135994172961688, w0=70.00000000000013, w1=14.087836973747459\n",
      "SubSGD iter. 120/499: loss=11.77230982189964, w0=70.70000000000013, w1=14.750981290790506\n",
      "SubSGD iter. 121/499: loss=0.4272135915403936, w0=71.40000000000013, w1=15.382146630785359\n",
      "SubSGD iter. 122/499: loss=1.879037905810975, w0=70.70000000000013, w1=14.886520370316527\n",
      "SubSGD iter. 123/499: loss=0.3912641985209291, w0=71.40000000000013, w1=15.151305479507295\n",
      "SubSGD iter. 124/499: loss=6.395327920741387, w0=72.10000000000014, w1=14.297459209082026\n",
      "SubSGD iter. 125/499: loss=0.2903702666326069, w0=71.40000000000013, w1=14.04717809261997\n",
      "SubSGD iter. 126/499: loss=1.5665531439815084, w0=72.10000000000014, w1=14.276299784906728\n",
      "SubSGD iter. 127/499: loss=8.0023947209161, w0=72.80000000000014, w1=14.03531987024008\n",
      "SubSGD iter. 128/499: loss=7.408661921986322, w0=72.10000000000014, w1=14.060617635628923\n",
      "SubSGD iter. 129/499: loss=1.6143220580974003, w0=72.80000000000014, w1=14.588673451139572\n",
      "SubSGD iter. 130/499: loss=2.0740411333819324, w0=73.50000000000014, w1=13.910326455165029\n",
      "SubSGD iter. 131/499: loss=1.1827862681368089, w0=72.80000000000014, w1=13.621829190607613\n",
      "SubSGD iter. 132/499: loss=5.407169345000362, w0=72.10000000000014, w1=14.461856762067423\n",
      "SubSGD iter. 133/499: loss=4.263225845624341, w0=72.80000000000014, w1=13.107899868034195\n",
      "SubSGD iter. 134/499: loss=1.6330194262669977, w0=73.50000000000014, w1=13.635955683544845\n",
      "SubSGD iter. 135/499: loss=0.5407290535237479, w0=72.80000000000014, w1=12.552080286853542\n",
      "SubSGD iter. 136/499: loss=10.20953042763341, w0=72.10000000000014, w1=12.983872865136963\n",
      "SubSGD iter. 137/499: loss=3.253143708987558, w0=72.80000000000014, w1=12.41086278212234\n",
      "SubSGD iter. 138/499: loss=4.1476483833111075, w0=72.10000000000014, w1=12.850551187207525\n",
      "SubSGD iter. 139/499: loss=1.258229117701461, w0=72.80000000000014, w1=13.079672879494284\n",
      "SubSGD iter. 140/499: loss=3.2523004713368806, w0=73.50000000000014, w1=14.17229739382008\n",
      "SubSGD iter. 141/499: loss=2.1669918454630874, w0=74.20000000000014, w1=14.315627984414595\n",
      "SubSGD iter. 142/499: loss=7.373828212524579, w0=73.50000000000014, w1=14.8511476672408\n",
      "SubSGD iter. 143/499: loss=6.8891626681943805, w0=74.20000000000014, w1=14.111814867796856\n",
      "SubSGD iter. 144/499: loss=0.0824551106661886, w0=73.50000000000014, w1=13.94600860822936\n",
      "SubSGD iter. 145/499: loss=7.300341285964066, w0=72.80000000000014, w1=14.331292748557487\n",
      "SubSGD iter. 146/499: loss=8.411452952421477, w0=72.10000000000014, w1=14.611221740993951\n",
      "SubSGD iter. 147/499: loss=8.097917871244675, w0=72.80000000000014, w1=15.623900082346719\n",
      "SubSGD iter. 148/499: loss=13.269671410449988, w0=73.50000000000014, w1=15.968368329217187\n",
      "SubSGD iter. 149/499: loss=4.035241842606226, w0=74.20000000000014, w1=15.63200537154111\n",
      "SubSGD iter. 150/499: loss=1.0246230921478485, w0=73.50000000000014, w1=15.28829275035772\n",
      "SubSGD iter. 151/499: loss=6.04008605460043, w0=72.80000000000014, w1=15.570840569443945\n",
      "SubSGD iter. 152/499: loss=2.9708211297005818, w0=72.10000000000014, w1=16.304954892444933\n",
      "SubSGD iter. 153/499: loss=0.6356411030663836, w0=71.40000000000013, w1=16.655291825813325\n",
      "SubSGD iter. 154/499: loss=13.374828765094776, w0=72.10000000000014, w1=16.036150670398943\n",
      "SubSGD iter. 155/499: loss=4.642668208552671, w0=71.40000000000013, w1=16.44722585494117\n",
      "SubSGD iter. 156/499: loss=2.40344650783139, w0=72.10000000000014, w1=16.4125349955997\n",
      "SubSGD iter. 157/499: loss=1.5695460613643988, w0=71.40000000000013, w1=17.253953505325644\n",
      "SubSGD iter. 158/499: loss=7.805460119701465, w0=72.10000000000014, w1=18.43475820602554\n",
      "SubSGD iter. 159/499: loss=6.317079971469852, w0=72.80000000000014, w1=18.47204551764341\n",
      "SubSGD iter. 160/499: loss=6.600140985089695, w0=72.10000000000014, w1=18.293693396263595\n",
      "SubSGD iter. 161/499: loss=0.1403092139539126, w0=71.40000000000013, w1=17.784507552317343\n",
      "SubSGD iter. 162/499: loss=7.504321242770764, w0=72.10000000000014, w1=17.230400478320764\n",
      "SubSGD iter. 163/499: loss=0.175368713071137, w0=71.40000000000013, w1=17.001278786034007\n",
      "SubSGD iter. 164/499: loss=1.1086737129331397, w0=70.70000000000013, w1=16.73649367684324\n",
      "SubSGD iter. 165/499: loss=11.680445729908449, w0=71.40000000000013, w1=15.997160877399295\n",
      "SubSGD iter. 166/499: loss=11.58739960039695, w0=72.10000000000014, w1=16.34512792378971\n",
      "SubSGD iter. 167/499: loss=5.150012216974773, w0=72.80000000000014, w1=16.08969604268814\n",
      "SubSGD iter. 168/499: loss=4.32773204537763, w0=72.10000000000014, w1=16.64166747462582\n",
      "SubSGD iter. 169/499: loss=3.4943425981829392, w0=71.40000000000013, w1=17.177187157452025\n",
      "SubSGD iter. 170/499: loss=6.153632867320908, w0=72.10000000000014, w1=16.921755276350453\n",
      "SubSGD iter. 171/499: loss=8.213372915409124, w0=72.80000000000014, w1=17.035575603164766\n",
      "SubSGD iter. 172/499: loss=2.5216469793172394, w0=72.10000000000014, w1=16.770790493973998\n",
      "SubSGD iter. 173/499: loss=1.2234953384315617, w0=72.80000000000014, w1=15.644858080472826\n",
      "SubSGD iter. 174/499: loss=2.387855345810884, w0=72.10000000000014, w1=16.463516450278455\n",
      "SubSGD iter. 175/499: loss=120.67013252680378, w0=72.80000000000014, w1=13.090372772688777\n",
      "SubSGD iter. 176/499: loss=6.214957209428782, w0=72.10000000000014, w1=13.437456758308768\n",
      "SubSGD iter. 177/499: loss=6.831936621880388, w0=71.40000000000013, w1=13.176744536926904\n",
      "SubSGD iter. 178/499: loss=1.0553446961707493, w0=72.10000000000014, w1=13.781276642781064\n",
      "SubSGD iter. 179/499: loss=10.1327986492361, w0=72.80000000000014, w1=13.162135487366681\n",
      "SubSGD iter. 180/499: loss=2.89242884976143, w0=73.50000000000014, w1=13.671321331312933\n",
      "SubSGD iter. 181/499: loss=1.4664981421937142, w0=72.80000000000014, w1=13.421040214850876\n",
      "SubSGD iter. 182/499: loss=7.581804481657244, w0=72.10000000000014, w1=13.968202517084746\n",
      "SubSGD iter. 183/499: loss=1.4031493298514945, w0=71.40000000000013, w1=14.39921320183336\n",
      "SubSGD iter. 184/499: loss=0.3732481577812905, w0=72.10000000000014, w1=14.649494318295417\n",
      "SubSGD iter. 185/499: loss=0.33506750079573067, w0=71.40000000000013, w1=14.517170791941579\n",
      "SubSGD iter. 186/499: loss=11.717354489981489, w0=70.70000000000013, w1=14.637062605429911\n",
      "SubSGD iter. 187/499: loss=3.5230305024488047, w0=70.00000000000013, w1=14.458710484050096\n",
      "SubSGD iter. 188/499: loss=1.8298512965587292, w0=69.30000000000013, w1=13.80152778085511\n",
      "SubSGD iter. 189/499: loss=5.858879954297905, w0=70.00000000000013, w1=13.890055294344236\n",
      "SubSGD iter. 190/499: loss=9.451487517825129, w0=70.70000000000013, w1=12.863123096751703\n",
      "SubSGD iter. 191/499: loss=7.352388645003373, w0=71.40000000000013, w1=13.02857588868433\n",
      "SubSGD iter. 192/499: loss=2.3015457894506284, w0=72.10000000000014, w1=13.933750991871394\n",
      "SubSGD iter. 193/499: loss=6.8760705992970514, w0=72.80000000000014, w1=14.086644748985975\n",
      "SubSGD iter. 194/499: loss=4.756538752372009, w0=73.50000000000014, w1=14.942866801490513\n",
      "SubSGD iter. 195/499: loss=7.070988611677691, w0=74.20000000000014, w1=14.592928870010196\n",
      "SubSGD iter. 196/499: loss=2.901364369234358, w0=73.50000000000014, w1=14.000610908742019\n",
      "SubSGD iter. 197/499: loss=1.9575295862639734, w0=74.20000000000014, w1=14.879783220972826\n",
      "SubSGD iter. 198/499: loss=7.444761354684964, w0=73.50000000000014, w1=15.437906924721865\n",
      "SubSGD iter. 199/499: loss=1.3524221415623288, w0=74.20000000000014, w1=15.31089247949384\n",
      "SubSGD iter. 200/499: loss=3.5800274609085108, w0=73.50000000000014, w1=14.104876189723392\n",
      "SubSGD iter. 201/499: loss=2.770781149103179, w0=74.20000000000014, w1=13.531866106708769\n",
      "SubSGD iter. 202/499: loss=4.371118855435206, w0=73.50000000000014, w1=14.433498371121527\n",
      "SubSGD iter. 203/499: loss=4.8074773026280155, w0=72.80000000000014, w1=13.265167570604058\n",
      "SubSGD iter. 204/499: loss=0.8457488844623811, w0=73.50000000000014, w1=13.230476711262588\n",
      "SubSGD iter. 205/499: loss=5.2961638631197445, w0=74.20000000000014, w1=14.242915049129008\n",
      "SubSGD iter. 206/499: loss=8.441111986793118, w0=74.90000000000015, w1=13.623773893714626\n",
      "SubSGD iter. 207/499: loss=8.506395412456008, w0=74.20000000000014, w1=13.867800718423776\n",
      "SubSGD iter. 208/499: loss=4.206222130054869, w0=73.50000000000014, w1=14.52468168617674\n",
      "SubSGD iter. 209/499: loss=5.831861390008726, w0=72.80000000000014, w1=14.645186237655782\n",
      "SubSGD iter. 210/499: loss=1.5455829084701662, w0=72.10000000000014, w1=14.052868276387604\n",
      "SubSGD iter. 211/499: loss=0.8232382308636517, w0=72.80000000000014, w1=14.163168309848396\n",
      "SubSGD iter. 212/499: loss=0.9423553006456302, w0=72.10000000000014, w1=13.91288719338634\n",
      "SubSGD iter. 213/499: loss=2.7728458721160223, w0=72.80000000000014, w1=13.72942141433592\n",
      "SubSGD iter. 214/499: loss=0.7585633400474165, w0=73.50000000000014, w1=13.629144315095711\n",
      "SubSGD iter. 215/499: loss=102.24447363654654, w0=74.20000000000014, w1=10.858844618206723\n",
      "SubSGD iter. 216/499: loss=0.37737534378834425, w0=73.50000000000014, w1=11.443157155752614\n",
      "SubSGD iter. 217/499: loss=8.17718123514338, w0=74.20000000000014, w1=12.161320479783802\n",
      "SubSGD iter. 218/499: loss=0.6089559786263976, w0=73.50000000000014, w1=12.196011339125272\n",
      "SubSGD iter. 219/499: loss=11.991820329889244, w0=72.80000000000014, w1=12.139701724079968\n",
      "SubSGD iter. 220/499: loss=7.630858879506157, w0=72.10000000000014, w1=12.550776908622193\n",
      "SubSGD iter. 221/499: loss=12.754146673843678, w0=71.40000000000013, w1=12.670668722110525\n",
      "SubSGD iter. 222/499: loss=4.0989902236003815, w0=72.10000000000014, w1=13.049208505467256\n",
      "SubSGD iter. 223/499: loss=1.5748656022713305, w0=72.80000000000014, w1=13.95438360865432\n",
      "SubSGD iter. 224/499: loss=4.156372869035195, w0=72.10000000000014, w1=13.297200905459334\n",
      "SubSGD iter. 225/499: loss=7.240711920266264, w0=72.80000000000014, w1=14.02916549896697\n",
      "SubSGD iter. 226/499: loss=5.141579042287319, w0=72.10000000000014, w1=14.534183230791292\n",
      "SubSGD iter. 227/499: loss=4.909121155991855, w0=72.80000000000014, w1=15.39040528329583\n",
      "SubSGD iter. 228/499: loss=3.498173003183446, w0=72.10000000000014, w1=16.231823793021775\n",
      "SubSGD iter. 229/499: loss=3.807884538727116, w0=71.40000000000013, w1=16.76734347584798\n",
      "SubSGD iter. 230/499: loss=3.484698026365564, w0=72.10000000000014, w1=17.016941938535158\n",
      "SubSGD iter. 231/499: loss=3.7133399669222626, w0=71.40000000000013, w1=16.66191446100533\n",
      "SubSGD iter. 232/499: loss=1.1923784137579574, w0=70.70000000000013, w1=17.274712632006743\n",
      "SubSGD iter. 233/499: loss=9.600362779506717, w0=70.00000000000013, w1=17.21840301696144\n",
      "SubSGD iter. 234/499: loss=0.06288585224407939, w0=69.30000000000013, w1=16.48412727346964\n",
      "SubSGD iter. 235/499: loss=4.405164266216175, w0=70.00000000000013, w1=15.540620789932518\n",
      "SubSGD iter. 236/499: loss=8.625098655554893, w0=70.70000000000013, w1=15.6935145470471\n",
      "SubSGD iter. 237/499: loss=7.181523611015564, w0=70.00000000000013, w1=14.735379456145292\n",
      "SubSGD iter. 238/499: loss=6.6813743125865415, w0=70.70000000000013, w1=14.750687122857428\n",
      "SubSGD iter. 239/499: loss=6.09943282968446, w0=71.40000000000013, w1=14.177677039842806\n",
      "SubSGD iter. 240/499: loss=1.7490276488283598, w0=72.10000000000014, w1=14.923662693430202\n",
      "SubSGD iter. 241/499: loss=1.845674794904717, w0=71.40000000000013, w1=15.349448840720942\n",
      "SubSGD iter. 242/499: loss=14.804728179098916, w0=72.10000000000014, w1=15.69391708759141\n",
      "SubSGD iter. 243/499: loss=6.614683203511547, w0=71.40000000000013, w1=15.91117409177116\n",
      "SubSGD iter. 244/499: loss=8.353990221646107, w0=72.10000000000014, w1=15.480842503310424\n",
      "SubSGD iter. 245/499: loss=0.5925414890801335, w0=71.40000000000013, w1=16.13772347106339\n",
      "SubSGD iter. 246/499: loss=4.6323672444498385, w0=72.10000000000014, w1=17.01697143695956\n",
      "SubSGD iter. 247/499: loss=0.4239678722474167, w0=71.40000000000013, w1=15.65160198429595\n",
      "SubSGD iter. 248/499: loss=0.1827323630943738, w0=72.10000000000014, w1=14.56180696122273\n",
      "SubSGD iter. 249/499: loss=1.846148690805066, w0=72.80000000000014, w1=14.76424385126373\n",
      "SubSGD iter. 250/499: loss=7.3118000278000395, w0=73.50000000000014, w1=14.077549215757594\n",
      "SubSGD iter. 251/499: loss=7.923123790923157, w0=74.20000000000014, w1=14.690541712941155\n",
      "SubSGD iter. 252/499: loss=1.9855350824114169, w0=73.50000000000014, w1=14.895375232816377\n",
      "SubSGD iter. 253/499: loss=7.129375134951509, w0=72.80000000000014, w1=15.442537535050247\n",
      "SubSGD iter. 254/499: loss=4.049445468190825, w0=73.50000000000014, w1=16.066025657336425\n",
      "SubSGD iter. 255/499: loss=0.40700454969859123, w0=72.80000000000014, w1=16.967657921749183\n",
      "SubSGD iter. 256/499: loss=2.951066130119969, w0=73.50000000000014, w1=17.434240047715758\n",
      "SubSGD iter. 257/499: loss=1.748808539607694, w0=74.20000000000014, w1=17.861757132933175\n",
      "SubSGD iter. 258/499: loss=4.872822222262528, w0=73.50000000000014, w1=17.020566964708532\n",
      "SubSGD iter. 259/499: loss=0.7110965228620216, w0=74.20000000000014, w1=17.470746710914828\n",
      "SubSGD iter. 260/499: loss=2.354038049566441, w0=73.50000000000014, w1=17.24162501862807\n",
      "SubSGD iter. 261/499: loss=3.299588146420305, w0=72.80000000000014, w1=16.976839909437302\n",
      "SubSGD iter. 262/499: loss=1.4392180009253792, w0=73.50000000000014, w1=17.427019655643598\n",
      "SubSGD iter. 263/499: loss=1.540790908877213, w0=72.80000000000014, w1=18.267047227103408\n",
      "SubSGD iter. 264/499: loss=3.5159677376635017, w0=72.10000000000014, w1=18.70724248181038\n",
      "SubSGD iter. 265/499: loss=1.9785833818637855, w0=72.80000000000014, w1=18.97738093210005\n",
      "SubSGD iter. 266/499: loss=1.7763042861422917, w0=72.10000000000014, w1=19.16154171041837\n",
      "SubSGD iter. 267/499: loss=9.772434909800133, w0=72.80000000000014, w1=18.765565467196538\n",
      "SubSGD iter. 268/499: loss=5.333236380976572, w0=73.50000000000014, w1=18.510133586094966\n",
      "SubSGD iter. 269/499: loss=13.722008556382868, w0=72.80000000000014, w1=18.523888782070458\n",
      "SubSGD iter. 270/499: loss=0.4432664926203529, w0=73.50000000000014, w1=19.156163210552148\n",
      "SubSGD iter. 271/499: loss=0.1228267441909594, w0=74.20000000000014, w1=20.168841551904915\n",
      "SubSGD iter. 272/499: loss=7.9309435954602066, w0=74.90000000000015, w1=19.927861637238266\n",
      "SubSGD iter. 273/499: loss=10.11997090964131, w0=74.20000000000014, w1=19.022686534051203\n",
      "SubSGD iter. 274/499: loss=3.781343687676909, w0=73.50000000000014, w1=18.010248196184783\n",
      "SubSGD iter. 275/499: loss=8.089335178857311, w0=74.20000000000014, w1=19.249564497162783\n",
      "SubSGD iter. 276/499: loss=10.311865207891174, w0=74.90000000000015, w1=18.562869861656647\n",
      "SubSGD iter. 277/499: loss=0.5003627550921763, w0=74.20000000000014, w1=19.219750829409612\n",
      "SubSGD iter. 278/499: loss=5.739717765599067, w0=74.90000000000015, w1=19.333571156223925\n",
      "SubSGD iter. 279/499: loss=12.314628715817307, w0=74.20000000000014, w1=17.97081976083218\n",
      "SubSGD iter. 280/499: loss=2.6451276829789663, w0=74.90000000000015, w1=18.136272552764805\n",
      "SubSGD iter. 281/499: loss=4.324463252886275, w0=74.20000000000014, w1=17.847775288207387\n",
      "SubSGD iter. 282/499: loss=6.884428905702677, w0=74.90000000000015, w1=16.99392901778212\n",
      "SubSGD iter. 283/499: loss=2.910100753968358, w0=74.20000000000014, w1=17.812587387587747\n",
      "SubSGD iter. 284/499: loss=6.635652667341162, w0=74.90000000000015, w1=16.601164499661568\n",
      "SubSGD iter. 285/499: loss=1.11772105515454, w0=74.20000000000014, w1=17.502796764074326\n",
      "SubSGD iter. 286/499: loss=1.6650044009031433, w0=73.50000000000014, w1=16.993610920128074\n",
      "SubSGD iter. 287/499: loss=5.622922947675633, w0=72.80000000000014, w1=17.378895060456202\n",
      "SubSGD iter. 288/499: loss=2.545364030837689, w0=72.10000000000014, w1=16.61753810983713\n",
      "SubSGD iter. 289/499: loss=5.064990865019439, w0=72.80000000000014, w1=16.782990901769757\n",
      "SubSGD iter. 290/499: loss=6.419058377722152, w0=73.50000000000014, w1=17.446135218812806\n",
      "SubSGD iter. 291/499: loss=5.301230855579988, w0=74.20000000000014, w1=17.711618921037037\n",
      "SubSGD iter. 292/499: loss=1.1596590204554431, w0=74.90000000000015, w1=16.768112437499916\n",
      "SubSGD iter. 293/499: loss=10.606607376125467, w0=75.60000000000015, w1=17.112580684370382\n",
      "SubSGD iter. 294/499: loss=1.4933688524796196, w0=74.90000000000015, w1=16.46643345098316\n",
      "SubSGD iter. 295/499: loss=11.113387238288261, w0=74.20000000000014, w1=16.73791363296574\n",
      "SubSGD iter. 296/499: loss=2.0450385042704653, w0=74.90000000000015, w1=17.456076956996927\n",
      "SubSGD iter. 297/499: loss=0.608240561436844, w0=75.60000000000015, w1=18.174240281028116\n",
      "SubSGD iter. 298/499: loss=2.2297126187795158, w0=74.90000000000015, w1=18.992898650833745\n",
      "SubSGD iter. 299/499: loss=2.7551696485631254, w0=74.20000000000014, w1=18.882598617372953\n",
      "SubSGD iter. 300/499: loss=4.742973164022921, w0=73.50000000000014, w1=19.229682602992945\n",
      "SubSGD iter. 301/499: loss=1.113032797145678, w0=72.80000000000014, w1=19.177522257463234\n",
      "SubSGD iter. 302/499: loss=1.4704920895674007, w0=72.10000000000014, w1=18.321300204958696\n",
      "SubSGD iter. 303/499: loss=3.3250552423534145, w0=71.40000000000013, w1=17.480110036734054\n",
      "SubSGD iter. 304/499: loss=3.680264704440134, w0=72.10000000000014, w1=18.094771017864442\n",
      "SubSGD iter. 305/499: loss=0.868822221295261, w0=71.40000000000013, w1=17.565070043802326\n",
      "SubSGD iter. 306/499: loss=4.307934498750463, w0=72.10000000000014, w1=17.21195429223501\n",
      "SubSGD iter. 307/499: loss=0.2995935008984958, w0=72.80000000000014, w1=17.5556669134184\n",
      "SubSGD iter. 308/499: loss=5.588921794209497, w0=72.10000000000014, w1=16.854732915983647\n",
      "SubSGD iter. 309/499: loss=0.9962908534101729, w0=71.40000000000013, w1=16.56623565142623\n",
      "SubSGD iter. 310/499: loss=9.406586007734177, w0=72.10000000000014, w1=16.762580244943457\n",
      "SubSGD iter. 311/499: loss=2.746643970940525, w0=71.40000000000013, w1=17.185819669715972\n",
      "SubSGD iter. 312/499: loss=3.4641150872905655, w0=70.70000000000013, w1=17.53099675744097\n",
      "SubSGD iter. 313/499: loss=10.230530280307548, w0=71.40000000000013, w1=17.742161152007174\n",
      "SubSGD iter. 314/499: loss=0.706318212423021, w0=72.10000000000014, w1=17.008046829006187\n",
      "SubSGD iter. 315/499: loss=0.545261981873935, w0=71.40000000000013, w1=16.559387790221475\n",
      "SubSGD iter. 316/499: loss=12.758013681077173, w0=72.10000000000014, w1=17.798704091199475\n",
      "SubSGD iter. 317/499: loss=0.7712265938505993, w0=71.40000000000013, w1=17.756377534595984\n",
      "SubSGD iter. 318/499: loss=2.6275804064787707, w0=70.70000000000013, w1=17.837191840084696\n",
      "SubSGD iter. 319/499: loss=5.223811790419077, w0=70.00000000000013, w1=16.780790608705807\n",
      "SubSGD iter. 320/499: loss=9.782941921140804, w0=70.70000000000013, w1=15.926944338280538\n",
      "SubSGD iter. 321/499: loss=5.893465225937774, w0=71.40000000000013, w1=16.12346442105904\n",
      "SubSGD iter. 322/499: loss=2.6972258537899805, w0=70.70000000000013, w1=16.613147954455627\n",
      "SubSGD iter. 323/499: loss=6.64781344641176, w0=71.40000000000013, w1=16.357716073354055\n",
      "SubSGD iter. 324/499: loss=0.8884679967247564, w0=72.10000000000014, w1=16.971630486731637\n",
      "SubSGD iter. 325/499: loss=11.328157832871199, w0=72.80000000000014, w1=18.210946787709638\n",
      "SubSGD iter. 326/499: loss=0.4729671247856828, w0=73.50000000000014, w1=18.863003685152094\n",
      "SubSGD iter. 327/499: loss=6.378332471861597, w0=74.20000000000014, w1=19.14113065395378\n",
      "SubSGD iter. 328/499: loss=5.8620644847487, w0=73.50000000000014, w1=18.37977370333471\n",
      "SubSGD iter. 329/499: loss=0.6579903285565223, w0=72.80000000000014, w1=18.17733681329371\n",
      "SubSGD iter. 330/499: loss=1.7236249838282305, w0=72.10000000000014, w1=18.37808635034296\n",
      "SubSGD iter. 331/499: loss=6.483412993006311, w0=71.40000000000013, w1=17.29421095365166\n",
      "SubSGD iter. 332/499: loss=7.206281631913882, w0=72.10000000000014, w1=17.90720345083522\n",
      "SubSGD iter. 333/499: loss=5.754301008007587, w0=71.40000000000013, w1=16.823328054143918\n",
      "SubSGD iter. 334/499: loss=8.92937722769338, w0=72.10000000000014, w1=16.93714838095823\n",
      "SubSGD iter. 335/499: loss=6.603790812182325, w0=71.40000000000013, w1=16.962446146347073\n",
      "SubSGD iter. 336/499: loss=0.5432215609404523, w0=70.70000000000013, w1=16.712165029885018\n",
      "SubSGD iter. 337/499: loss=0.23185974603789816, w0=70.00000000000013, w1=15.977889286393218\n",
      "SubSGD iter. 338/499: loss=2.2617374870200706, w0=69.30000000000013, w1=16.260437105479443\n",
      "SubSGD iter. 339/499: loss=7.348024206568269, w0=70.00000000000013, w1=16.27574477219158\n",
      "SubSGD iter. 340/499: loss=2.67832594950751, w0=70.70000000000013, w1=16.070911252316357\n",
      "SubSGD iter. 341/499: loss=11.267515514795157, w0=71.40000000000013, w1=15.671453479929777\n",
      "SubSGD iter. 342/499: loss=0.6995856353760388, w0=70.70000000000013, w1=16.111141885014963\n",
      "SubSGD iter. 343/499: loss=5.553921367173871, w0=71.40000000000013, w1=16.73463000730114\n",
      "SubSGD iter. 344/499: loss=0.7766713902452338, w0=70.70000000000013, w1=17.438134925692\n",
      "SubSGD iter. 345/499: loss=0.5424627191805769, w0=71.40000000000013, w1=16.868174801156016\n",
      "SubSGD iter. 346/499: loss=9.48935676722897, w0=70.70000000000013, w1=15.910039710254209\n",
      "SubSGD iter. 347/499: loss=0.6178685533091794, w0=70.00000000000013, w1=15.278874370259356\n",
      "SubSGD iter. 348/499: loss=5.542892162101623, w0=70.70000000000013, w1=14.600527374284813\n",
      "SubSGD iter. 349/499: loss=0.3861917696990389, w0=71.40000000000013, w1=14.169516689536199\n",
      "SubSGD iter. 350/499: loss=2.05292479667186, w0=72.10000000000014, w1=13.565750619569684\n",
      "SubSGD iter. 351/499: loss=3.764625402833012, w0=71.40000000000013, w1=14.655545642642904\n",
      "SubSGD iter. 352/499: loss=6.965807858258692, w0=70.70000000000013, w1=15.133593845069027\n",
      "SubSGD iter. 353/499: loss=0.9857420017063916, w0=70.00000000000013, w1=15.529030637012033\n",
      "SubSGD iter. 354/499: loss=14.859423938941617, w0=70.70000000000013, w1=16.65416059714234\n",
      "SubSGD iter. 355/499: loss=1.2888161426949267, w0=70.00000000000013, w1=16.02299525714749\n",
      "SubSGD iter. 356/499: loss=0.8543806036453887, w0=70.70000000000013, w1=15.20433688734186\n",
      "SubSGD iter. 357/499: loss=10.014594718072232, w0=71.40000000000013, w1=15.867481204384907\n",
      "SubSGD iter. 358/499: loss=3.3847476466986564, w0=72.10000000000014, w1=16.662650929502256\n",
      "SubSGD iter. 359/499: loss=1.123294507696329, w0=71.40000000000013, w1=16.04351753355759\n",
      "SubSGD iter. 360/499: loss=5.160738120244048, w0=72.10000000000014, w1=16.240037616336092\n",
      "SubSGD iter. 361/499: loss=1.35353714529964, w0=72.80000000000014, w1=16.749223460282344\n",
      "SubSGD iter. 362/499: loss=0.016874958461116307, w0=73.50000000000014, w1=16.54438994040712\n",
      "SubSGD iter. 363/499: loss=0.46181602325728477, w0=74.20000000000014, w1=16.444112841166913\n",
      "SubSGD iter. 364/499: loss=4.467777963131326, w0=73.50000000000014, w1=15.851794879898735\n",
      "SubSGD iter. 365/499: loss=5.027560656824292, w0=72.80000000000014, w1=14.795393648519845\n",
      "SubSGD iter. 366/499: loss=1.0004444792716072, w0=73.50000000000014, w1=14.429472917789447\n",
      "SubSGD iter. 367/499: loss=7.72313849377052, w0=72.80000000000014, w1=15.05225932460974\n",
      "SubSGD iter. 368/499: loss=1.0127823127868396, w0=72.10000000000014, w1=15.953891589022497\n",
      "SubSGD iter. 369/499: loss=5.5189860500923515, w0=72.80000000000014, w1=14.742468701096318\n",
      "SubSGD iter. 370/499: loss=0.07866904344760428, w0=72.10000000000014, w1=14.690308355566607\n",
      "SubSGD iter. 371/499: loss=12.387700523572988, w0=71.40000000000013, w1=14.810200169054939\n",
      "SubSGD iter. 372/499: loss=2.5882356581342734, w0=72.10000000000014, w1=13.626197968661048\n",
      "SubSGD iter. 373/499: loss=0.8842999268975333, w0=72.80000000000014, w1=13.022431898694535\n",
      "SubSGD iter. 374/499: loss=10.00335783928655, w0=72.10000000000014, w1=13.307606393243828\n",
      "SubSGD iter. 375/499: loss=5.4395697838809625, w0=71.40000000000013, w1=13.590154212330054\n",
      "SubSGD iter. 376/499: loss=6.756483135670585, w0=72.10000000000014, w1=13.062965573582828\n",
      "SubSGD iter. 377/499: loss=11.779173196103315, w0=72.80000000000014, w1=13.50289001646701\n",
      "SubSGD iter. 378/499: loss=15.31609753267334, w0=73.50000000000014, w1=14.628019976597317\n",
      "SubSGD iter. 379/499: loss=4.135235621798174, w0=74.20000000000014, w1=14.793472768529943\n",
      "SubSGD iter. 380/499: loss=0.8725054215667782, w0=74.90000000000015, w1=14.1151257725554\n",
      "SubSGD iter. 381/499: loss=4.036454782633314, w0=75.60000000000015, w1=14.268019529669981\n",
      "SubSGD iter. 382/499: loss=8.193159623919676, w0=74.90000000000015, w1=14.892835629480928\n",
      "SubSGD iter. 383/499: loss=8.883157908452276, w0=74.20000000000014, w1=15.40133674722076\n",
      "SubSGD iter. 384/499: loss=6.7702667149444835, w0=74.90000000000015, w1=14.662003947776817\n",
      "SubSGD iter. 385/499: loss=8.619375028192849, w0=74.20000000000014, w1=14.463243081958293\n",
      "SubSGD iter. 386/499: loss=3.746266920272639, w0=73.50000000000014, w1=15.006259543323392\n",
      "SubSGD iter. 387/499: loss=2.685613883053577, w0=72.80000000000014, w1=15.356596476691783\n",
      "SubSGD iter. 388/499: loss=4.387276717168277, w0=73.50000000000014, w1=16.088561070199418\n",
      "SubSGD iter. 389/499: loss=0.01307765180382603, w0=72.80000000000014, w1=14.723191617535807\n",
      "SubSGD iter. 390/499: loss=7.873736825352665, w0=73.50000000000014, w1=13.696259419943274\n",
      "SubSGD iter. 391/499: loss=5.572193451129522, w0=74.20000000000014, w1=13.171626495231536\n",
      "SubSGD iter. 392/499: loss=4.959644249482999, w0=73.50000000000014, w1=14.115132978768656\n",
      "SubSGD iter. 393/499: loss=0.100949157675565, w0=74.20000000000014, w1=13.344309912700469\n",
      "SubSGD iter. 394/499: loss=3.6062873272107083, w0=74.90000000000015, w1=13.996366810142923\n",
      "SubSGD iter. 395/499: loss=0.9277817424313355, w0=74.20000000000014, w1=13.155176641918281\n",
      "SubSGD iter. 396/499: loss=11.349278674925259, w0=73.50000000000014, w1=13.440351136467575\n",
      "SubSGD iter. 397/499: loss=4.742271236949605, w0=72.80000000000014, w1=13.83578792841058\n",
      "SubSGD iter. 398/499: loss=6.634831808870821, w0=72.10000000000014, w1=14.246863112952806\n",
      "SubSGD iter. 399/499: loss=0.8220325493492595, w0=71.40000000000013, w1=14.240435232074459\n",
      "SubSGD iter. 400/499: loss=0.5355276028533638, w0=70.70000000000013, w1=14.671445916823073\n",
      "SubSGD iter. 401/499: loss=2.3523624742454246, w0=71.40000000000013, w1=15.120104955607783\n",
      "SubSGD iter. 402/499: loss=3.399123865660094, w0=70.70000000000013, w1=15.609788489004371\n",
      "SubSGD iter. 403/499: loss=5.800184346053463, w0=70.00000000000013, w1=15.889717481440835\n",
      "SubSGD iter. 404/499: loss=0.2113721363456733, w0=70.70000000000013, w1=15.705556703122515\n",
      "SubSGD iter. 405/499: loss=5.604582346385413, w0=71.40000000000013, w1=16.13307378833993\n",
      "SubSGD iter. 406/499: loss=1.2201555784940794, w0=70.70000000000013, w1=16.703033912875917\n",
      "SubSGD iter. 407/499: loss=1.2900197928201038, w0=70.00000000000013, w1=16.09850180702176\n",
      "SubSGD iter. 408/499: loss=4.1843598259332, w0=70.70000000000013, w1=15.722915968735188\n",
      "SubSGD iter. 409/499: loss=12.423725587333323, w0=71.40000000000013, w1=16.070883015125602\n",
      "SubSGD iter. 410/499: loss=1.1562582552348317, w0=70.70000000000013, w1=16.466319807068608\n",
      "SubSGD iter. 411/499: loss=3.5673273069208378, w0=70.00000000000013, w1=16.616776483519736\n",
      "SubSGD iter. 412/499: loss=4.564291879922422, w0=70.70000000000013, w1=17.262923716906958\n",
      "SubSGD iter. 413/499: loss=2.6595675675864783, w0=70.00000000000013, w1=17.444787560482126\n",
      "SubSGD iter. 414/499: loss=4.721232617108612, w0=70.70000000000013, w1=18.061174509319375\n",
      "SubSGD iter. 415/499: loss=14.170294769019279, w0=71.40000000000013, w1=18.40564275618984\n",
      "SubSGD iter. 416/499: loss=2.8520951117706446, w0=72.10000000000014, w1=17.748761788436877\n",
      "SubSGD iter. 417/499: loss=2.956717942706831, w0=72.80000000000014, w1=18.176278873654294\n",
      "SubSGD iter. 418/499: loss=3.6652656986044505, w0=73.50000000000014, w1=17.376448139591584\n",
      "SubSGD iter. 419/499: loss=2.2505534234214224, w0=72.80000000000014, w1=17.244124613237744\n",
      "SubSGD iter. 420/499: loss=1.9912562940138656, w0=72.10000000000014, w1=17.444874150286996\n",
      "SubSGD iter. 421/499: loss=0.24556952831665058, w0=71.40000000000013, w1=17.21575245800024\n",
      "SubSGD iter. 422/499: loss=2.6392054785348336, w0=72.10000000000014, w1=16.089820044499067\n",
      "SubSGD iter. 423/499: loss=0.08356186830869206, w0=72.80000000000014, w1=16.617875860009715\n",
      "SubSGD iter. 424/499: loss=1.5025543064713247, w0=72.10000000000014, w1=17.50670768417621\n",
      "SubSGD iter. 425/499: loss=6.433445990005694, w0=72.80000000000014, w1=18.169852001219258\n",
      "SubSGD iter. 426/499: loss=6.677383352702812, w0=72.10000000000014, w1=18.85045117887534\n",
      "SubSGD iter. 427/499: loss=5.996554488550899, w0=71.40000000000013, w1=18.672099057495526\n",
      "SubSGD iter. 428/499: loss=10.05129483039049, w0=72.10000000000014, w1=18.24176746903479\n",
      "SubSGD iter. 429/499: loss=2.0703581277249015, w0=72.80000000000014, w1=18.973732062542425\n",
      "SubSGD iter. 430/499: loss=0.2375748748175539, w0=72.10000000000014, w1=18.32167516509997\n",
      "SubSGD iter. 431/499: loss=0.1505960270598905, w0=72.80000000000014, w1=18.43197519856076\n",
      "SubSGD iter. 432/499: loss=1.3867613603140398, w0=73.50000000000014, w1=19.055463320846936\n",
      "SubSGD iter. 433/499: loss=3.3820339971009687, w0=74.20000000000014, w1=17.965668297773718\n",
      "SubSGD iter. 434/499: loss=3.8048941590152623, w0=74.90000000000015, w1=19.146472998473612\n",
      "SubSGD iter. 435/499: loss=1.5111688642555379, w0=74.20000000000014, w1=19.16505245737249\n",
      "SubSGD iter. 436/499: loss=8.600454712405849, w0=73.50000000000014, w1=18.464118459937737\n",
      "SubSGD iter. 437/499: loss=1.1339400744642347, w0=72.80000000000014, w1=18.085578676581004\n",
      "SubSGD iter. 438/499: loss=6.572390015174889, w0=72.10000000000014, w1=18.302835680760754\n",
      "SubSGD iter. 439/499: loss=3.264492355677419, w0=71.40000000000013, w1=18.383649986249466\n",
      "SubSGD iter. 440/499: loss=5.479389887401311, w0=72.10000000000014, w1=17.589280641758688\n",
      "SubSGD iter. 441/499: loss=2.499407444834503, w0=72.80000000000014, w1=16.49948561868547\n",
      "SubSGD iter. 442/499: loss=3.9257464101338826, w0=73.50000000000014, w1=15.821138622710926\n",
      "SubSGD iter. 443/499: loss=4.506849917152451, w0=72.80000000000014, w1=15.4661111451811\n",
      "SubSGD iter. 444/499: loss=0.46698547290611714, w0=72.10000000000014, w1=16.40961762871822\n",
      "SubSGD iter. 445/499: loss=2.7230678023989157, w0=72.80000000000014, w1=16.38565178891513\n",
      "SubSGD iter. 446/499: loss=4.419799778173058, w0=73.50000000000014, w1=16.551104580847756\n",
      "SubSGD iter. 447/499: loss=5.530365013712945, w0=72.80000000000014, w1=16.83365239993398\n",
      "SubSGD iter. 448/499: loss=4.182764452006211, w0=73.50000000000014, w1=17.84633074128675\n",
      "SubSGD iter. 449/499: loss=12.328287636908641, w0=74.20000000000014, w1=17.227189585872367\n",
      "SubSGD iter. 450/499: loss=1.663817471868768, w0=73.50000000000014, w1=17.175029240342656\n",
      "SubSGD iter. 451/499: loss=5.594947319309632, w0=74.20000000000014, w1=16.590716702796765\n",
      "SubSGD iter. 452/499: loss=1.3205433703789566, w0=73.50000000000014, w1=15.75737912023719\n",
      "SubSGD iter. 453/499: loss=9.88484651803293, w0=72.80000000000014, w1=15.208984616479379\n",
      "SubSGD iter. 454/499: loss=8.191735140656846, w0=72.10000000000014, w1=14.948272395097515\n",
      "SubSGD iter. 455/499: loss=8.518785567651861, w0=71.40000000000013, w1=15.233446889646808\n",
      "SubSGD iter. 456/499: loss=5.965362013067761, w0=70.70000000000013, w1=15.25874465503565\n",
      "SubSGD iter. 457/499: loss=4.083665463368945, w0=71.40000000000013, w1=15.23477881523256\n",
      "SubSGD iter. 458/499: loss=8.13110963376419, w0=70.70000000000013, w1=15.915377992888642\n",
      "SubSGD iter. 459/499: loss=9.336143890156656, w0=71.40000000000013, w1=15.519401749666809\n",
      "SubSGD iter. 460/499: loss=7.64320420966537, w0=70.70000000000013, w1=14.561266658765001\n",
      "SubSGD iter. 461/499: loss=16.995590951852336, w0=71.40000000000013, w1=15.800582959743002\n",
      "SubSGD iter. 462/499: loss=3.800298380165202, w0=72.10000000000014, w1=16.07072141003267\n",
      "SubSGD iter. 463/499: loss=6.012153028269154, w0=72.80000000000014, w1=16.097094060657493\n",
      "SubSGD iter. 464/499: loss=3.913894889907894, w0=72.10000000000014, w1=14.734342665265746\n",
      "SubSGD iter. 465/499: loss=0.43491618728404546, w0=72.80000000000014, w1=14.624168548385006\n",
      "SubSGD iter. 466/499: loss=6.2270780437528686, w0=72.10000000000014, w1=15.009452688713134\n",
      "SubSGD iter. 467/499: loss=5.998443758499249, w0=71.40000000000013, w1=15.517953806452967\n",
      "SubSGD iter. 468/499: loss=0.7960028162504926, w0=70.70000000000013, w1=15.957642211538152\n",
      "SubSGD iter. 469/499: loss=6.820617703794454, w0=71.40000000000013, w1=16.48182276298987\n",
      "SubSGD iter. 470/499: loss=5.899892531137297, w0=72.10000000000014, w1=16.2263908818883\n",
      "SubSGD iter. 471/499: loss=2.8954919306918896, w0=72.80000000000014, w1=16.099376436660275\n",
      "SubSGD iter. 472/499: loss=0.8448441158015072, w0=73.50000000000014, w1=16.477916220017008\n",
      "SubSGD iter. 473/499: loss=8.233001395232549, w0=74.20000000000014, w1=16.917840662901188\n",
      "SubSGD iter. 474/499: loss=0.2775082216520133, w0=74.90000000000015, w1=17.569897560343644\n",
      "SubSGD iter. 475/499: loss=5.584220482335304, w0=74.20000000000014, w1=18.10541724316985\n",
      "SubSGD iter. 476/499: loss=3.600751313426933, w0=73.50000000000014, w1=18.61043497499417\n",
      "SubSGD iter. 477/499: loss=5.921531128629965, w0=72.80000000000014, w1=18.255407497464343\n",
      "SubSGD iter. 478/499: loss=2.405512718797482, w0=72.10000000000014, w1=18.005126381002288\n",
      "SubSGD iter. 479/499: loss=0.949710054982873, w0=72.80000000000014, w1=17.894952264121546\n",
      "SubSGD iter. 480/499: loss=6.435331351065855, w0=72.10000000000014, w1=16.811076867430245\n",
      "SubSGD iter. 481/499: loss=10.575000865071829, w0=71.40000000000013, w1=16.20567371598626\n",
      "SubSGD iter. 482/499: loss=1.257821805101912, w0=72.10000000000014, w1=16.00084019611104\n",
      "SubSGD iter. 483/499: loss=4.283946564655899, w0=71.40000000000013, w1=16.836879053383853\n",
      "SubSGD iter. 484/499: loss=12.374613516094328, w0=72.10000000000014, w1=15.80994685579132\n",
      "SubSGD iter. 485/499: loss=1.9797885355819815, w0=71.40000000000013, w1=16.166672374147158\n",
      "SubSGD iter. 486/499: loss=0.6505135251365886, w0=72.10000000000014, w1=15.735661689398544\n",
      "SubSGD iter. 487/499: loss=8.467832632736275, w0=71.40000000000013, w1=15.187267185640733\n",
      "SubSGD iter. 488/499: loss=1.6982882591474038, w0=70.70000000000013, w1=14.852039130337479\n",
      "SubSGD iter. 489/499: loss=0.3620670924458551, w0=70.00000000000013, w1=15.9418341534107\n",
      "SubSGD iter. 490/499: loss=1.5847753687154267, w0=70.70000000000013, w1=17.034458667736494\n",
      "SubSGD iter. 491/499: loss=0.5780226377393589, w0=71.40000000000013, w1=16.19304015801055\n",
      "SubSGD iter. 492/499: loss=1.3209729941402273, w0=70.70000000000013, w1=16.896545076401413\n",
      "SubSGD iter. 493/499: loss=3.5610483676336884, w0=71.40000000000013, w1=15.953038592864292\n",
      "SubSGD iter. 494/499: loss=11.672256188896903, w0=70.70000000000013, w1=15.966793788839782\n",
      "SubSGD iter. 495/499: loss=2.966216070784931, w0=70.00000000000013, w1=16.249341607926006\n",
      "SubSGD iter. 496/499: loss=6.502954457197362, w0=70.70000000000013, w1=16.445861690704508\n",
      "SubSGD iter. 497/499: loss=5.569531087614905, w0=71.40000000000013, w1=17.0887913894683\n",
      "SubSGD iter. 498/499: loss=0.7008419239752399, w0=72.10000000000014, w1=16.270133019662673\n",
      "SubSGD iter. 499/499: loss=6.2396013094221985, w0=72.80000000000014, w1=15.685820482116782\n",
      "SubSGD: execution time=0.013 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e50d363c5e849c5ac6cc08c6b7c3080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
